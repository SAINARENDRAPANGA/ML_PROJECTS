{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPYgivn9qIaFMfZpCJB1Bjq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Name :- Sai Narendra Panga \n","#Reg.No :- 21BCE9868"],"metadata":{"id":"ZeigcG1SE6pD"}},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p1LezDGhDUaB","executionInfo":{"status":"ok","timestamp":1682051392253,"user_tz":-330,"elapsed":431,"user":{"displayName":"PANGA SAI NARENDRA 21BCE9868","userId":"11781088589423733827"}},"outputId":"50d9e590-4000-4827-9769-fc3881ecef7b"},"outputs":[{"output_type":"stream","name":"stdout","text":["-----------Epoch- 1 Starts----------\n","Input: \n","[[0.66666667 1.        ]\n"," [0.33333333 0.55555556]\n"," [1.         0.66666667]]\n","Actual Output: \n","[[0.92]\n"," [0.86]\n"," [0.89]]\n","Predicted Output: \n"," [[0.83639168]\n"," [0.82053342]\n"," [0.83157814]]\n","-----------Epoch- 1 Ends----------\n","\n","-----------Epoch- 2 Starts----------\n","Input: \n","[[0.66666667 1.        ]\n"," [0.33333333 0.55555556]\n"," [1.         0.66666667]]\n","Actual Output: \n","[[0.92]\n"," [0.86]\n"," [0.89]]\n","Predicted Output: \n"," [[0.83707323]\n"," [0.8211928 ]\n"," [0.83225842]]\n","-----------Epoch- 2 Ends----------\n","\n","-----------Epoch- 3 Starts----------\n","Input: \n","[[0.66666667 1.        ]\n"," [0.33333333 0.55555556]\n"," [1.         0.66666667]]\n","Actual Output: \n","[[0.92]\n"," [0.86]\n"," [0.89]]\n","Predicted Output: \n"," [[0.83774295]\n"," [0.82184101]\n"," [0.83292698]]\n","-----------Epoch- 3 Ends----------\n","\n","-----------Epoch- 4 Starts----------\n","Input: \n","[[0.66666667 1.        ]\n"," [0.33333333 0.55555556]\n"," [1.         0.66666667]]\n","Actual Output: \n","[[0.92]\n"," [0.86]\n"," [0.89]]\n","Predicted Output: \n"," [[0.83840112]\n"," [0.82247832]\n"," [0.83358411]]\n","-----------Epoch- 4 Ends----------\n","\n","-----------Epoch- 5 Starts----------\n","Input: \n","[[0.66666667 1.        ]\n"," [0.33333333 0.55555556]\n"," [1.         0.66666667]]\n","Actual Output: \n","[[0.92]\n"," [0.86]\n"," [0.89]]\n","Predicted Output: \n"," [[0.83904803]\n"," [0.82310501]\n"," [0.83423009]]\n","-----------Epoch- 5 Ends----------\n","\n","Input: \n","[[0.66666667 1.        ]\n"," [0.33333333 0.55555556]\n"," [1.         0.66666667]]\n","Actual Output: \n","[[0.92]\n"," [0.86]\n"," [0.89]]\n","Predicted Output: \n"," [[0.83904803]\n"," [0.82310501]\n"," [0.83423009]]\n"]}],"source":["import numpy as np\n","\n","X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n","y = np.array(([92], [86], [89]), dtype=float)\n","X = X/np.amax(X,axis=0) #maximum of X array longitudinally\n","y = y/100\n","\n","#Sigmoid Function\n","def sigmoid (x):\n","    return 1/(1 + np.exp(-x))\n","\n","#Derivative of Sigmoid Function\n","def derivatives_sigmoid(x):\n","    return x * (1 - x)\n","\n","#Variable initialization\n","epoch=5 #Setting training iterations\n","lr=0.1 #Setting learning rate\n","\n","inputlayer_neurons = 2 #number of features in data set\n","hiddenlayer_neurons = 3 #number of hidden layers neurons\n","output_neurons = 1 #number of neurons at output layer\n","#weight and bias initialization\n","\n","wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n","bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n","wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n","bout=np.random.uniform(size=(1,output_neurons))\n","\n","#draws a random range of numbers uniformly of dim x*y\n","for i in range(epoch):\n","    #Forward Propogation\n","    hinp1=np.dot(X,wh)\n","    hinp=hinp1 + bh\n","    hlayer_act = sigmoid(hinp)\n","    outinp1=np.dot(hlayer_act,wout)\n","    outinp= outinp1+bout\n","    output = sigmoid(outinp)\n","    \n","    #Backpropagation\n","    EO = y-output\n","    outgrad = derivatives_sigmoid(output)\n","    d_output = EO * outgrad\n","    EH = d_output.dot(wout.T)\n","    hiddengrad = derivatives_sigmoid(hlayer_act)#how much hidden layer wts contributed to error\n","    d_hiddenlayer = EH * hiddengrad\n","    \n","    wout += hlayer_act.T.dot(d_output) *lr   # dotproduct of nextlayererror and currentlayerop\n","    wh += X.T.dot(d_hiddenlayer) *lr\n","    \n","    print (\"-----------Epoch-\", i+1, \"Starts----------\")\n","    print(\"Input: \\n\" + str(X)) \n","    print(\"Actual Output: \\n\" + str(y))\n","    print(\"Predicted Output: \\n\" ,output)\n","    print (\"-----------Epoch-\", i+1, \"Ends----------\\n\")\n","        \n","print(\"Input: \\n\" + str(X)) \n","print(\"Actual Output: \\n\" + str(y))\n","print(\"Predicted Output: \\n\" ,output)"]}]}